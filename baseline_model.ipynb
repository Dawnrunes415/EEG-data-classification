{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51c533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293e794",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d6c99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 23, 256) (656, 23, 256) (8282,) (656,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('processed_eeg_data.npz')\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1ea78",
   "metadata": {},
   "source": [
    "### Calculating Hurst Scores\n",
    "\n",
    "These statistics were used by the original baseline model, implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23dd5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hurst\n",
    "from hurst import compute_Hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22a4ebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 samples\n",
      "Processed 100 samples\n",
      "Processed 200 samples\n",
      "Processed 300 samples\n",
      "Processed 400 samples\n",
      "Processed 500 samples\n",
      "Processed 600 samples\n",
      "Processed 700 samples\n",
      "Processed 800 samples\n",
      "Processed 900 samples\n",
      "Processed 1000 samples\n",
      "Processed 1100 samples\n",
      "Processed 1200 samples\n",
      "Processed 1300 samples\n",
      "Processed 1400 samples\n",
      "Processed 1500 samples\n",
      "Processed 1600 samples\n",
      "Processed 1700 samples\n",
      "Processed 1800 samples\n",
      "Processed 1900 samples\n",
      "Processed 2000 samples\n",
      "Processed 2100 samples\n",
      "Processed 2200 samples\n",
      "Processed 2300 samples\n",
      "Processed 2400 samples\n",
      "Processed 2500 samples\n",
      "Processed 2600 samples\n",
      "Processed 2700 samples\n",
      "Processed 2800 samples\n",
      "Processed 2900 samples\n",
      "Processed 3000 samples\n",
      "Processed 3100 samples\n",
      "Processed 3200 samples\n",
      "Processed 3300 samples\n",
      "Processed 3400 samples\n",
      "Processed 3500 samples\n",
      "Processed 3600 samples\n",
      "Processed 3700 samples\n",
      "Processed 3800 samples\n",
      "Processed 3900 samples\n",
      "Processed 4000 samples\n",
      "Processed 4100 samples\n",
      "Processed 4200 samples\n",
      "Processed 4300 samples\n",
      "Processed 4400 samples\n",
      "Processed 4500 samples\n",
      "Processed 4600 samples\n",
      "Processed 4700 samples\n",
      "Processed 4800 samples\n",
      "Processed 4900 samples\n",
      "Processed 5000 samples\n",
      "Processed 5100 samples\n",
      "Processed 5200 samples\n",
      "Processed 5300 samples\n",
      "Processed 5400 samples\n",
      "Processed 5500 samples\n",
      "Processed 5600 samples\n",
      "Processed 5700 samples\n",
      "Processed 5800 samples\n",
      "Processed 5900 samples\n",
      "Processed 6000 samples\n",
      "Processed 6100 samples\n",
      "Processed 6200 samples\n",
      "Processed 6300 samples\n",
      "Processed 6400 samples\n",
      "Processed 6500 samples\n",
      "Processed 6600 samples\n",
      "Processed 6700 samples\n",
      "Processed 6800 samples\n",
      "Processed 6900 samples\n",
      "Processed 7000 samples\n",
      "Processed 7100 samples\n",
      "Processed 7200 samples\n",
      "Processed 7300 samples\n",
      "Processed 7400 samples\n",
      "Processed 7500 samples\n",
      "Processed 7600 samples\n",
      "Processed 7700 samples\n",
      "Processed 7800 samples\n",
      "Processed 7900 samples\n",
      "Processed 8000 samples\n",
      "Processed 8100 samples\n",
      "Processed 8200 samples\n",
      "(8282, 23) (8282, 23)\n"
     ]
    }
   ],
   "source": [
    "# New array to store Hurst exponent and constant for each sample\n",
    "hurst_train_exp = np.empty((X_train.shape[0], X_train.shape[1]))\n",
    "hurst_train_const = np.empty((X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "\n",
    "# Takes about 5 minutes to run\n",
    "for i in range(X_train.shape[0]):\n",
    "    for j in range(X_train.shape[1]):\n",
    "        hurst_train_exp[i, j], hurst_train_const[i,j], _ = compute_Hc(X_train[i, j], kind='change', simplified=True)\n",
    "    # Status messages\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} samples\")\n",
    "\n",
    "print(hurst_train_exp.shape, hurst_train_const.shape) # >>> 8282 by 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7d808f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 samples\n",
      "Processed 100 samples\n",
      "Processed 200 samples\n",
      "Processed 300 samples\n",
      "Processed 400 samples\n",
      "Processed 500 samples\n",
      "Processed 600 samples\n",
      "(656, 23) (8282, 23)\n"
     ]
    }
   ],
   "source": [
    "# Same for the validation set\n",
    "hurst_val_exp = np.empty((X_val.shape[0], X_val.shape[1]))\n",
    "hurst_val_const = np.empty((X_val.shape[0], X_val.shape[1]))\n",
    "\n",
    "# 21 seconds to run\n",
    "for i in range(X_val.shape[0]):\n",
    "    for j in range(X_val.shape[1]):\n",
    "        hurst_val_exp[i, j], hurst_val_const[i,j], _ = compute_Hc(X_val[i, j], kind='change', simplified=True)\n",
    "    # Status messages\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} samples\")\n",
    "\n",
    "print(hurst_val_exp.shape, hurst_train_const.shape) # >>> 656 by 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b99b6",
   "metadata": {},
   "source": [
    "### Wavelet analysis\n",
    "Additional stats used by the original baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3beff45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken directly from the original notebook\n",
    "def statisticsForWavelet(coefs):\n",
    "    n5 = np.nanpercentile(coefs, 5)\n",
    "    n25 = np.nanpercentile(coefs, 25)\n",
    "    n75 = np.nanpercentile(coefs, 75)\n",
    "    n95 = np.nanpercentile(coefs, 95)\n",
    "    median = np.nanpercentile(coefs, 50)\n",
    "    mean = np.nanmean(coefs)\n",
    "    std = np.nanstd(coefs)\n",
    "    var = np.nanvar(coefs)\n",
    "    rms = np.nanmean(np.sqrt(coefs**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 samples\n",
      "Processed 100 samples\n",
      "Processed 200 samples\n",
      "Processed 300 samples\n",
      "Processed 400 samples\n",
      "Processed 500 samples\n",
      "Processed 600 samples\n",
      "Processed 700 samples\n",
      "Processed 800 samples\n",
      "Processed 900 samples\n",
      "Processed 1000 samples\n",
      "Processed 1100 samples\n",
      "Processed 1200 samples\n",
      "Processed 1300 samples\n",
      "Processed 1400 samples\n",
      "Processed 1500 samples\n",
      "Processed 1600 samples\n",
      "Processed 1700 samples\n",
      "Processed 1800 samples\n",
      "Processed 1900 samples\n",
      "Processed 2000 samples\n",
      "Processed 2100 samples\n",
      "Processed 2200 samples\n",
      "Processed 2300 samples\n",
      "Processed 2400 samples\n",
      "Processed 2500 samples\n",
      "Processed 2600 samples\n",
      "Processed 2700 samples\n",
      "Processed 2800 samples\n",
      "Processed 2900 samples\n",
      "Processed 3000 samples\n",
      "Processed 3100 samples\n",
      "Processed 3200 samples\n",
      "Processed 3300 samples\n",
      "Processed 3400 samples\n",
      "Processed 3500 samples\n",
      "Processed 3600 samples\n",
      "Processed 3700 samples\n",
      "Processed 3800 samples\n",
      "Processed 3900 samples\n",
      "Processed 4000 samples\n",
      "Processed 4100 samples\n",
      "Processed 4200 samples\n",
      "Processed 4300 samples\n",
      "Processed 4400 samples\n",
      "Processed 4500 samples\n",
      "Processed 4600 samples\n",
      "Processed 4700 samples\n",
      "Processed 4800 samples\n",
      "Processed 4900 samples\n",
      "Processed 5000 samples\n",
      "Processed 5100 samples\n",
      "Processed 5200 samples\n",
      "Processed 5300 samples\n",
      "Processed 5400 samples\n",
      "Processed 5500 samples\n",
      "Processed 5600 samples\n",
      "Processed 5700 samples\n",
      "Processed 5800 samples\n",
      "Processed 5900 samples\n",
      "Processed 6000 samples\n",
      "Processed 6100 samples\n",
      "Processed 6200 samples\n",
      "Processed 6300 samples\n",
      "Processed 6400 samples\n",
      "Processed 6500 samples\n",
      "Processed 6600 samples\n",
      "Processed 6700 samples\n",
      "Processed 6800 samples\n",
      "Processed 6900 samples\n",
      "Processed 7000 samples\n",
      "Processed 7100 samples\n",
      "Processed 7200 samples\n",
      "Processed 7300 samples\n",
      "Processed 7400 samples\n",
      "Processed 7500 samples\n",
      "Processed 7600 samples\n",
      "Processed 7700 samples\n",
      "Processed 7800 samples\n",
      "Processed 7900 samples\n",
      "Processed 8000 samples\n",
      "Processed 8100 samples\n",
      "Processed 8200 samples\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "train_wavelets = []\n",
    "\n",
    "# Takes about 9 minutes to run\n",
    "# Perform wavelet decomposition for each channel in each sample in the training set\n",
    "for i in range(X_train.shape[0]):\n",
    "    group = []\n",
    "    for j in range(X_train.shape[1]):\n",
    "        # Function returns a list of wavelet coefficients\n",
    "        # Using 'db4' as in the original notebook\n",
    "        coefficents = pywt.wavedec(X_train[i, j], 'db4')\n",
    "        \n",
    "        subgroup = []\n",
    "        for k in coefficents:\n",
    "            # Get statistics for each wavelet decomposition level\n",
    "            statistics = statisticsForWavelet(k)\n",
    "            subgroup.append(statistics)\n",
    "        group.append(subgroup)\n",
    "    train_wavelets.append(group)\n",
    "    \n",
    "    # Status messages\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3669e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 23, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "train_wavelets = np.array(train_wavelets)\n",
    "print(train_wavelets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cdfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 samples\n",
      "Processed 100 samples\n",
      "Processed 200 samples\n",
      "Processed 300 samples\n",
      "Processed 400 samples\n",
      "Processed 500 samples\n",
      "Processed 600 samples\n"
     ]
    }
   ],
   "source": [
    "# Same as above for the validation set\n",
    "val_wavelets = []\n",
    "\n",
    "\n",
    "# Will take about 40s to run\n",
    "# Perform wavelet decomposition for each channel in each sample in the training set\n",
    "for i in range(X_val.shape[0]):\n",
    "    group = []\n",
    "    for j in range(X_val.shape[1]):\n",
    "        # Function returns a list of wavelet coefficients\n",
    "        # Using 'db4' as in the original notebook\n",
    "        coefficents = pywt.wavedec(X_val[i, j], 'db4')\n",
    "        \n",
    "        subgroup = []\n",
    "        for k in coefficents:\n",
    "            # Get statistics for each wavelet decomposition level\n",
    "            statistics = statisticsForWavelet(k)\n",
    "            subgroup.append(statistics)\n",
    "        group.append(subgroup)\n",
    "    val_wavelets.append(group)\n",
    "    \n",
    "    # Status messages\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0155e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 23, 6, 9)\n",
      "(656, 23, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "train_wavelets = np.array(train_wavelets)\n",
    "val_wavelets = np.array(val_wavelets)\n",
    "\n",
    "print(train_wavelets.shape) # >>> 8282 by 23 by 6 by 9\n",
    "print(val_wavelets.shape) # >>> 656 by 23 by 6 by 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c30ca3",
   "metadata": {},
   "source": [
    "### SVM Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da394a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 23, 54) (8282, 23, 1) (8282, 23, 1) (8282, 23, 256)\n",
      "(656, 23, 54) (656, 23, 1) (656, 23, 1) (656, 23, 256)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the wavelet features\n",
    "train_wavelets = train_wavelets.reshape(train_wavelets.shape[0], train_wavelets.shape[1], -1)\n",
    "val_wavelets = val_wavelets.reshape(val_wavelets.shape[0], train_wavelets.shape[1], -1)\n",
    "\n",
    "# Unflatten the Hurst exponent and constant arrays to match the shape of the other features\n",
    "hurst_train_exp = np.reshape(hurst_train_exp, (-1, 23, 1))\n",
    "hurst_train_const = np.reshape(hurst_train_const, (-1, 23, 1))\n",
    "\n",
    "hurst_val_exp = np.reshape(hurst_val_exp, (-1, 23, 1))\n",
    "hurst_val_const = np.reshape(hurst_val_const, (-1, 23, 1))\n",
    "\n",
    "print(train_wavelets.shape, hurst_train_exp.shape, hurst_train_const.shape, X_train.shape)\n",
    "print(val_wavelets.shape, hurst_val_exp.shape, hurst_val_const.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3586b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack() got multiple values for argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Concatenating the hurst, wavelet, and original features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhurst_train_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhurst_train_const\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_wavelets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(X_val, hurst_val_exp, hurst_val_const, val_wavelets, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: stack() got multiple values for argument 'axis'"
     ]
    }
   ],
   "source": [
    "# Concatenating the hurst, wavelet, and original features\n",
    "X_train = np.concatenate(X_train, hurst_train_exp, hurst_train_const, train_wavelets, axis=-1)\n",
    "X_val = np.concatenate(X_val, hurst_val_exp, hurst_val_const, val_wavelets, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ac41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classfier (a type of SVM), uses linear decision boundary\n",
    "clf = svm.SVC(kernel=\"linear\",probability=True)\n",
    "\n",
    "# Flatten the datasets to 2D as SVC expects 2D array input\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "y_train = y_train.reshape(y_train.shape[0], -1)\n",
    "\n",
    "# Fits SVM on the training data, \n",
    "# I don't know why they are doing this as the next line re-fits the model.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# k-fold cross validation with 10 folds\n",
    "y_pred = cross_val_predict(clf, X_val, y_val, cv=10)\n",
    "print(\"All features are included\\n\", classification_report(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
